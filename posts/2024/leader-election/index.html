<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Leader Election 折腾小记 | STARRY-S' Blog</title>
<meta name=keywords content="Kubernetes,Leader Election"><meta name=description content="最近好忙，有很多想写博客的东西都没时间写，五一去了佛山的 HiFurry，本来想着整理点照片水一篇博客但没时间也没精力，所以最后想写的东西就都咽肚里就饭吃了。"><meta name=author content="STARRY-S"><link rel=canonical href=https://blog.starry-s.moe/posts/2024/leader-election/><link crossorigin=anonymous href=/assets/css/stylesheet.8cd7e7b23ccf6a26b5a720d9989e983050dcfab2e6de4beda008dc78cf697d32.css integrity="sha256-jNfnsjzPaia1pyDZmJ6YMFDc+rLm3kvtoAjceM9pfTI=" rel="preload stylesheet" as=style><link rel=icon href=https://blog.starry-s.moe/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=https://blog.starry-s.moe/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://blog.starry-s.moe/favicon-32x32.png><link rel=apple-touch-icon href=https://blog.starry-s.moe/apple-touch-icon.png><link rel=mask-icon href=https://blog.starry-s.moe/apple-touch-icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=zh href=https://blog.starry-s.moe/posts/2024/leader-election/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://fastly.jsdelivr.net/npm/aplayer/dist/APlayer.min.css><script src=https://fastly.jsdelivr.net/npm/aplayer/dist/APlayer.min.js></script><script src=https://fastly.jsdelivr.net/npm/meting/dist/Meting.min.js></script><script>var meting_api="https://api.injahow.cn/meting/?server=:server&type=:type&id=:id&auth=:auth&r=:r"</script><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#da532c"><meta name=theme-color content="#232323"><meta property="og:title" content="Leader Election 折腾小记"><meta property="og:description" content="最近好忙，有很多想写博客的东西都没时间写，五一去了佛山的 HiFurry，本来想着整理点照片水一篇博客但没时间也没精力，所以最后想写的东西就都咽肚里就饭吃了。"><meta property="og:type" content="article"><meta property="og:url" content="https://blog.starry-s.moe/posts/2024/leader-election/"><meta property="og:image" content="https://blog.starry-s.moe/avatar.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-06-12T23:58:26+08:00"><meta property="article:modified_time" content="2024-06-12T23:58:26+08:00"><meta property="og:site_name" content="STARRY-S' Blog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://blog.starry-s.moe/avatar.png"><meta name=twitter:title content="Leader Election 折腾小记"><meta name=twitter:description content="最近好忙，有很多想写博客的东西都没时间写，五一去了佛山的 HiFurry，本来想着整理点照片水一篇博客但没时间也没精力，所以最后想写的东西就都咽肚里就饭吃了。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://blog.starry-s.moe/posts/"},{"@type":"ListItem","position":2,"name":"Leader Election 折腾小记","item":"https://blog.starry-s.moe/posts/2024/leader-election/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Leader Election 折腾小记","name":"Leader Election 折腾小记","description":"最近好忙，有很多想写博客的东西都没时间写，五一去了佛山的 HiFurry，本来想着整理点照片水一篇博客但没时间也没精力，所以最后想写的东西就都咽肚里就饭吃了。\n","keywords":["Kubernetes","Leader Election"],"articleBody":"最近好忙，有很多想写博客的东西都没时间写，五一去了佛山的 HiFurry，本来想着整理点照片水一篇博客但没时间也没精力，所以最后想写的东西就都咽肚里就饭吃了。\n最近在折腾 Operator，就是用现成的框架写的 Controller。Operator 省去了重复且繁琐的使用 client-go 手搓 ClientSet、Informer、Lister、WorkQueue 等一大堆重复代码的步骤，只要基于已有的框架去写资源对象更新/删除时的业务处理逻辑就行了。\nLeader Election 是什么 当负载在 Kubernetes 运行时，通常会设置多个 Replicas 冗余副本，以实现高可用（HA），例如通常会将某些系统组件的 Replicas 设置为 2，就会创建两个对应的 Pods，通常这俩 Pod 会被调度到不同的节点上，在某个 Pod 挂掉时还能用另一个节点的 Pod。\nLeader Election 机制是由“领导人选举机制”抽象而来的，可以理解为在多个“候选者”中选取某一个作为 Leader。这里的候选者指的是负载创建的多个冗余 Pod，Leader Election 机制从中选取某一个 Pod 作为 Leader，其他 Pod 则处于“待命”状态，如果 Leader Pod 出现故障，则会重新选举一个 Leader Pod。\nKubernetes 使用 Lease 资源（译作：租约）作为 Leader Election 的锁。和常用的 Mutex 互斥锁不同，Lease 资源会被 Leader Pod 每隔几秒钟更新一次。如果长达一段时间 Lease 没有被更新，则说明 Leader 挂掉了，其他 Pods 会竞争，尝试更新这个 Lease 锁，而成功更新了 Lease 的 Pod 会成为新的 Leader，其余 Pod 则继续处于待命状态。\n大多数情况下，当某个资源发生更新时，我们不希望所有的冗余副本 Pod 都去处理某一个资源的更新，而是让某一个 Pod 去处理就可以了，不然会混乱（比如刷 Conflict 报错: the object has been modified; please apply your changes to the latest version and try again）。这时可以用到 Leader Election 机制，从多个冗余 Pod 中只选其中某一个 Pod 作为 Leader 处理资源更新，其余 Pod 只作为待命或其他用途。\n如果你的 Controller 没有 Leader Election 机制，通常只能强行设定其 Replicas 为 1，但如果有小聪明修改了冗余数值为 2，则会出现一些问题，日志会刷大量的 Conflict 报错之类的，所以更严谨的方式是为 Controller 添加 Leader Election，以允许多 Replicas 冗余。\n举个栗子 client-go 的样例代码中有 Leader Election 例子，所以直接拿这个 Example 做简单的介绍了，把这个 Example 代码拷贝下来在本地跑一下。\n首先你需要有一个 Kubernetes 集群用来调试，如果你觉得搭一个集群太麻烦，或者手里没有可供调试使用的集群的话，一个超级简单的方式是使用 K3d 在你的 Docker Runtime 中跑一个迷你版 K3s 集群。\n$ k3d cluster create example INFO[0000] Prep: Network INFO[0000] Created network 'k3d-example' ...... INFO[0012] Cluster 'example' created successfully! $ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME k3d-example-server-0 Ready control-plane,master 98s v1.28.8+k3s1 172.19.0.2 K3s v1.28.8+k3s1 6.9.3-arch1-1 containerd://1.7.11-k3s2 $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a4c1367c04a2 ghcr.io/k3d-io/k3d-proxy:5.6.3 \"/bin/sh -c nginx-pr…\" 2 minutes ago Up About a minute 80/tcp, 0.0.0.0:6443-\u003e6443/tcp k3d-example-serverlb 7c95a6ea069b rancher/k3s:v1.28.8-k3s1 \"/bin/k3d-entrypoint…\" 2 minutes ago Up 2 minutes k3d-example-server-0 按照样例的 README，在 3 个终端中运行 Leader Election 样例代码。\n$ go run main.go -kubeconfig=~/.kube/config -logtostderr=true -lease-lock-name=example -lease-lock-namespace=default -id=1 I0612 22:59:20.118613 27504 leaderelection.go:250] attempting to acquire leader lease default/example... I0612 22:59:20.124630 27504 leaderelection.go:260] successfully acquired lease default/example I0612 22:59:20.124696 27504 main.go:87] Controller loop... $ go run main.go -kubeconfig=~/.kube/config -logtostderr=true -lease-lock-name=example -lease-lock-namespace=default -id=2 I0612 22:59:32.692373 27815 leaderelection.go:250] attempting to acquire leader lease default/example... I0612 22:59:32.695277 27815 main.go:151] new leader elected: 1 $ go run main.go -kubeconfig=~/.kube/config -logtostderr=true -lease-lock-name=example -lease-lock-namespace=default -id=3 I0612 22:59:36.424251 28089 leaderelection.go:250] attempting to acquire leader lease default/example... I0612 22:59:36.427674 28089 main.go:151] new leader elected: 1 按顺序在 3 个终端中依次运行样例代码，可以看到 ID 为 1 的程序最先运行所以它成了 Leader，其余两个程序则在待命中。\n这时对 ID 1 的程序执行 Ctrl-C，发送 SIGINT 中断信号，让它 Context Canceled，ID 1 程序会释放 Lease 锁并结束运行，其余两个程序中的某一个则会重新竞争，其中一个变成 Leader。\n$ go run main.go -kubeconfig=~/.kube/config -logtostderr=true -lease-lock-name=example -lease-lock-namespace=default -id=3 I0612 22:59:36.424251 28089 leaderelection.go:250] attempting to acquire leader lease default/example... I0612 22:59:36.427674 28089 main.go:151] new leader elected: 1 I0612 23:02:56.584777 28089 leaderelection.go:260] successfully acquired lease default/example I0612 23:02:56.584866 28089 main.go:87] Controller loop... 查看样例程序代码，leaderelection.RunOrDie 的参数传递的 Config 定义了 Leader Election 机制的 Callback 回调函数以及租约相关的时间 Duration。\nCallbacks 回调函数分为：\nOnStartedLeading: 当该程序被选举为 Leader 时，执行此回调函数，通常该回调函数启动 Controller 的 Sync 逻辑等一些操作。 OnStoppedLeading: 当该程序不再是 Leader 时（可能是收到了 SIGINT 信号，Context Canceled 或程序出故障，很长一段时间没有去更新 Lease 锁），会执行此回调函数，执行一些资源释放等操作，然后直接 os.Exit 结束程序。 OnNewLeader: 当其他某个程序被选举为 Leader 时，会执行此函数，一般没什么用，可以不配置。 Config 的其他参数：\nLock: Lease Lock。 ReleaseOnCancel: 当 Context Cancel（当前的 Leader 结束运行）时，释放当前的 Lease 锁，使得其他 Pod 可以立即进行新一轮的选举。如果设置为 false 的话，当前 Leader 挂掉后其他 Pod 并不知道当前 Leader 已经挂掉了，只有过很长一段时间，发现 Lease 锁超过了 LeaseDuration 时间还没被更新，才会去强行的执行新一轮的选举。 LeaseDuration: 结合上方的 ReleaseOnCancel 的介绍，假设当前 Leader Pod 出故障了（例如被 SIGKILL 立即杀死，Context 来不及 Cancel，或者调试进入了 Breakpoint 断点，程序暂停），Lease 锁没被释放，但当前 Leader 出问题挂掉了，其他待命的 Pod 发现 Lease 锁已经超过 LeaseDuration 没有被更新，则会强行进行新一轮的选举，而原 Leader 如果还活着的话，也会执行 OnStoppedLeading 回调函数结束运行。 RenewDeadline: Leader 每隔一段时间会更新一次 Lease 锁。 RetryPeriod: 如果 Leader 更新 Lease 锁失败了，会在一段时间后重试。 所以有些小朋友在调试软件时，进入断点再恢复运行时会莫名其妙的结束运行，其实就是 Leader Election 机制搞的。所以如果想调试程序，可以临时把 LeaseDuration 设置长一些（例如好几天），这样调试断点恢复后，程序就不会被杀死了。\n杂谈 常用的 Operator 框架都支持 Leader Election，所以基本不用手写 RunOrDie 这部分代码，例如 Rancher 使用的 Wrangler 框架，当程序成为 Leader 时，直接执行 OnLeader 回调函数启动一系列业务逻辑。而当程序还没被选为 Leader 时，只初始化 Informer Cache 等初始化步骤，不启动 Sync 相关逻辑。\n通常 sample-controller 或其他简单的 Controller 在 Worker Start 执行完之后，会加一个 \u003c-ctx.Done() 阻塞（代码位置），遇到 Context Cancel 后直接结束运行。但如果加了 Leader Election 机制，当 Context Cancel 时是由 Leader Election 的 OnStoppedLeading 回调函数结束运行并释放 Lease 锁，所以 main 函数可以改为使用 select {} 阻塞，否则程序在 Context Cancel 时 Lease 锁还没来得及释放就由 main 函数结束运行了。\n所以读到这里，可以得出结论，就算设置数量特别多的 Replicas，实际上依旧只有一个 Controller Pod 在执行真正的 Sync 逻辑，而其他 Pod 只是在观望，或者只提供一些 Web Server 功能。如果想让多个冗余 Pod 分别 Sync 不同的资源更新，需要设计一个更复杂的锁，而这又会增加一定的 API Server 请求数量……\n","wordCount":"2366","inLanguage":"zh","image":"https://blog.starry-s.moe/avatar.png","datePublished":"2024-06-12T23:58:26+08:00","dateModified":"2024-06-12T23:58:26+08:00","author":{"@type":"Person","name":"STARRY-S"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.starry-s.moe/posts/2024/leader-election/"},"publisher":{"@type":"Organization","name":"STARRY-S' Blog","logo":{"@type":"ImageObject","url":"https://blog.starry-s.moe/favicon-32x32.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://blog.starry-s.moe/ accesskey=h title="Blog (Alt + H)"><img src=https://blog.starry-s.moe/apple-touch-icon.png alt aria-label=logo height=24>Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://blog.starry-s.moe/gallery/ title=相册><span>相册</span></a></li><li><a href=https://blog.starry-s.moe/archives/ title=归档><span>归档</span></a></li><li><a href=https://blog.starry-s.moe/tags/ title=标签><span>标签</span></a></li><li><a href=https://blog.starry-s.moe/categories/ title=分类><span>分类</span></a></li><li><a href=https://blog.starry-s.moe/search/ title="搜索 (Alt + /)" accesskey=/><span>搜索</span></a></li><li><a href=https://blog.starry-s.moe/projects/ title=项目><span>项目</span></a></li><li><a href=https://blog.starry-s.moe/about/ title=关于><span>关于</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://blog.starry-s.moe/>主页</a>&nbsp;»&nbsp;<a href=https://blog.starry-s.moe/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Leader Election 折腾小记</h1><div class=post-meta><span title='2024-06-12 23:58:26 +0800 +0800'>2024-06-12 23:58:26 +0800</span>&nbsp;·&nbsp;5 分钟&nbsp;·&nbsp;2366 字&nbsp;·&nbsp;STARRY-S&nbsp;|&nbsp;<a href=https://github.com/STARRY-S/blog/edit/main/content/posts/2024/leader-election/index.md rel="noopener noreferrer" target=_blank>Edit Text</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>目录</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#leader-election-是什么>Leader Election 是什么</a></li><li><a href=#举个栗子>举个栗子</a></li><li><a href=#杂谈>杂谈</a></li></ul></nav></div></details></div><div class=post-content><p>最近好忙，有很多想写博客的东西都没时间写，五一去了佛山的 HiFurry，本来想着整理点照片水一篇博客但没时间也没精力，所以最后想写的东西就都咽肚里就饭吃了。</p><meting-js server=netease type=song id=2055270589 theme=#233333></meting-js><hr><p>最近在折腾 Operator，就是用现成的框架写的 Controller。Operator 省去了重复且繁琐的使用 client-go 手搓 ClientSet、Informer、Lister、WorkQueue 等一大堆重复代码的步骤，只要基于已有的框架去写资源对象更新/删除时的业务处理逻辑就行了。</p><h2 id=leader-election-是什么>Leader Election 是什么<a hidden class=anchor aria-hidden=true href=#leader-election-是什么>#</a></h2><p>当负载在 Kubernetes 运行时，通常会设置多个 Replicas 冗余副本，以实现高可用（HA），例如通常会将某些系统组件的 Replicas 设置为 2，就会创建两个对应的 Pods，通常这俩 Pod 会被调度到不同的节点上，在某个 Pod 挂掉时还能用另一个节点的 Pod。</p><p>Leader Election 机制是由“领导人选举机制”抽象而来的，可以理解为在多个“候选者”中选取某一个作为 Leader。这里的候选者指的是负载创建的多个冗余 Pod，Leader Election 机制从中选取某一个 Pod 作为 Leader，其他 Pod 则处于“待命”状态，如果 Leader Pod 出现故障，则会重新选举一个 Leader Pod。</p><p>Kubernetes 使用 Lease 资源（译作：租约）作为 Leader Election 的锁。和常用的 Mutex 互斥锁不同，Lease 资源会被 Leader Pod 每隔几秒钟更新一次。如果长达一段时间 Lease 没有被更新，则说明 Leader 挂掉了，其他 Pods 会竞争，尝试更新这个 Lease 锁，而成功更新了 Lease 的 Pod 会成为新的 Leader，其余 Pod 则继续处于待命状态。</p><p>大多数情况下，当某个资源发生更新时，我们不希望所有的冗余副本 Pod 都去处理某一个资源的更新，而是让某一个 Pod 去处理就可以了，不然会混乱（比如刷 Conflict 报错: <code>the object has been modified; please apply your changes to the latest version and try again</code>）。这时可以用到 Leader Election 机制，从多个冗余 Pod 中只选其中某一个 Pod 作为 Leader 处理资源更新，其余 Pod 只作为待命或其他用途。</p><p>如果你的 Controller 没有 Leader Election 机制，通常只能强行设定其 Replicas 为 1，但如果有小聪明修改了冗余数值为 2，则会出现一些问题，日志会刷大量的 Conflict 报错之类的，所以更严谨的方式是为 Controller 添加 Leader Election，以允许多 Replicas 冗余。</p><h2 id=举个栗子>举个栗子<a hidden class=anchor aria-hidden=true href=#举个栗子>#</a></h2><p>client-go 的样例代码中有 <a href=https://github.com/kubernetes/client-go/blob/v0.30.1/examples/leader-election/main.go>Leader Election 例子</a>，所以直接拿这个 Example 做简单的介绍了，把这个 Example 代码拷贝下来在本地跑一下。</p><p>首先你需要有一个 Kubernetes 集群用来调试，如果你觉得搭一个集群太麻烦，或者手里没有可供调试使用的集群的话，一个超级简单的方式是使用 <a href=https://k3d.io/>K3d</a> 在你的 Docker Runtime 中跑一个迷你版 K3s 集群。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=gp>$</span> k3d cluster create example
</span></span><span class=line><span class=cl><span class=go>INFO[0000] Prep: Network
</span></span></span><span class=line><span class=cl><span class=go>INFO[0000] Created network &#39;k3d-example&#39;
</span></span></span><span class=line><span class=cl><span class=go>......
</span></span></span><span class=line><span class=cl><span class=go>INFO[0012] Cluster &#39;example&#39; created successfully!
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=gp>$</span> kubectl get nodes -o wide
</span></span><span class=line><span class=cl><span class=go>NAME                   STATUS   ROLES                  AGE   VERSION        INTERNAL-IP   EXTERNAL-IP   OS-IMAGE           KERNEL-VERSION   CONTAINER-RUNTIME
</span></span></span><span class=line><span class=cl><span class=go>k3d-example-server-0   Ready    control-plane,master   98s   v1.28.8+k3s1   172.19.0.2    &lt;none&gt;        K3s v1.28.8+k3s1   6.9.3-arch1-1    containerd://1.7.11-k3s2
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=gp>$</span> docker ps
</span></span><span class=line><span class=cl><span class=go>CONTAINER ID   IMAGE                            COMMAND                  CREATED          STATUS              PORTS                           NAMES
</span></span></span><span class=line><span class=cl><span class=go>a4c1367c04a2   ghcr.io/k3d-io/k3d-proxy:5.6.3   &#34;/bin/sh -c nginx-pr…&#34;   2 minutes ago    Up About a minute   80/tcp, 0.0.0.0:6443-&gt;6443/tcp  k3d-example-serverlb
</span></span></span><span class=line><span class=cl><span class=go>7c95a6ea069b   rancher/k3s:v1.28.8-k3s1         &#34;/bin/k3d-entrypoint…&#34;   2 minutes ago    Up 2 minutes                                        k3d-example-server-0
</span></span></span></code></pre></div><p>按照样例的 <a href=https://github.com/kubernetes/client-go/blob/v0.30.1/examples/leader-election/README.md>README</a>，在 3 个终端中运行 Leader Election 样例代码。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=gp>$</span> go run main.go -kubeconfig<span class=o>=</span>~/.kube/config -logtostderr<span class=o>=</span><span class=nb>true</span> -lease-lock-name<span class=o>=</span>example -lease-lock-namespace<span class=o>=</span>default -id<span class=o>=</span><span class=m>1</span>
</span></span><span class=line><span class=cl><span class=go>I0612 22:59:20.118613   27504 leaderelection.go:250] attempting to acquire leader lease default/example...
</span></span></span><span class=line><span class=cl><span class=go>I0612 22:59:20.124630   27504 leaderelection.go:260] successfully acquired lease default/example
</span></span></span><span class=line><span class=cl><span class=go>I0612 22:59:20.124696   27504 main.go:87] Controller loop...
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=gp>$</span> go run main.go -kubeconfig<span class=o>=</span>~/.kube/config -logtostderr<span class=o>=</span><span class=nb>true</span> -lease-lock-name<span class=o>=</span>example -lease-lock-namespace<span class=o>=</span>default -id<span class=o>=</span><span class=m>2</span>
</span></span><span class=line><span class=cl><span class=go>I0612 22:59:32.692373   27815 leaderelection.go:250] attempting to acquire leader lease default/example...
</span></span></span><span class=line><span class=cl><span class=go>I0612 22:59:32.695277   27815 main.go:151] new leader elected: 1
</span></span></span><span class=line><span class=cl><span class=go></span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=gp>$</span> go run main.go -kubeconfig<span class=o>=</span>~/.kube/config -logtostderr<span class=o>=</span><span class=nb>true</span> -lease-lock-name<span class=o>=</span>example -lease-lock-namespace<span class=o>=</span>default -id<span class=o>=</span><span class=m>3</span>
</span></span><span class=line><span class=cl><span class=go>I0612 22:59:36.424251   28089 leaderelection.go:250] attempting to acquire leader lease default/example...
</span></span></span><span class=line><span class=cl><span class=go>I0612 22:59:36.427674   28089 main.go:151] new leader elected: 1
</span></span></span></code></pre></div><p>按顺序在 3 个终端中依次运行样例代码，可以看到 ID 为 1 的程序最先运行所以它成了 Leader，其余两个程序则在待命中。</p><p>这时对 ID 1 的程序执行 Ctrl-C，发送 <code>SIGINT</code> 中断信号，让它 Context Canceled，ID 1 程序会释放 Lease 锁并结束运行，其余两个程序中的某一个则会重新竞争，其中一个变成 Leader。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=gp>$</span> go run main.go -kubeconfig<span class=o>=</span>~/.kube/config -logtostderr<span class=o>=</span><span class=nb>true</span> -lease-lock-name<span class=o>=</span>example -lease-lock-namespace<span class=o>=</span>default -id<span class=o>=</span><span class=m>3</span>
</span></span><span class=line><span class=cl><span class=go>I0612 22:59:36.424251   28089 leaderelection.go:250] attempting to acquire leader lease default/example...
</span></span></span><span class=line><span class=cl><span class=go>I0612 22:59:36.427674   28089 main.go:151] new leader elected: 1
</span></span></span><span class=line><span class=cl><span class=go>I0612 23:02:56.584777   28089 leaderelection.go:260] successfully acquired lease default/example
</span></span></span><span class=line><span class=cl><span class=go>I0612 23:02:56.584866   28089 main.go:87] Controller loop...
</span></span></span></code></pre></div><p>查看样例程序代码，<code>leaderelection.RunOrDie</code> 的参数传递的 Config 定义了 Leader Election 机制的 Callback 回调函数以及租约相关的时间 Duration。</p><p><code>Callbacks</code> 回调函数分为：</p><ul><li><code>OnStartedLeading</code>: 当该程序被选举为 Leader 时，执行此回调函数，通常该回调函数启动 Controller 的 Sync 逻辑等一些操作。</li><li><code>OnStoppedLeading</code>: 当该程序不再是 Leader 时（可能是收到了 <code>SIGINT</code> 信号，Context Canceled 或程序出故障，很长一段时间没有去更新 Lease 锁），会执行此回调函数，执行一些资源释放等操作，然后直接 <code>os.Exit</code> 结束程序。</li><li><code>OnNewLeader</code>: 当其他某个程序被选举为 Leader 时，会执行此函数，一般没什么用，可以不配置。</li></ul><p>Config 的其他参数：</p><ul><li><code>Lock</code>: Lease Lock。</li><li><code>ReleaseOnCancel</code>: 当 Context Cancel（当前的 Leader 结束运行）时，释放当前的 Lease 锁，使得其他 Pod 可以立即进行新一轮的选举。如果设置为 false 的话，当前 Leader 挂掉后其他 Pod 并不知道当前 Leader 已经挂掉了，只有过很长一段时间，发现 Lease 锁超过了 <code>LeaseDuration</code> 时间还没被更新，才会去强行的执行新一轮的选举。</li><li><code>LeaseDuration</code>: 结合上方的 <code>ReleaseOnCancel</code> 的介绍，假设当前 Leader Pod 出故障了（例如被 <code>SIGKILL</code> 立即杀死，Context 来不及 Cancel，或者调试进入了 Breakpoint 断点，程序暂停），Lease 锁没被释放，但当前 Leader 出问题挂掉了，其他待命的 Pod 发现 Lease 锁已经超过 <code>LeaseDuration</code> 没有被更新，则会强行进行新一轮的选举，而原 Leader 如果还活着的话，也会执行 <code>OnStoppedLeading</code> 回调函数结束运行。</li><li><code>RenewDeadline</code>: Leader 每隔一段时间会更新一次 Lease 锁。</li><li><code>RetryPeriod</code>: 如果 Leader 更新 Lease 锁失败了，会在一段时间后重试。</li></ul><p>所以有些小朋友在调试软件时，进入断点再恢复运行时会莫名其妙的结束运行，其实就是 Leader Election 机制搞的。所以如果想调试程序，可以临时把 <code>LeaseDuration</code> 设置长一些（例如好几天），这样调试断点恢复后，程序就不会被杀死了。</p><h2 id=杂谈>杂谈<a hidden class=anchor aria-hidden=true href=#杂谈>#</a></h2><p>常用的 Operator 框架都支持 Leader Election，所以基本不用手写 <code>RunOrDie</code> 这部分代码，例如 Rancher 使用的 <a href=https://github.com/rancher/wrangler/>Wrangler</a> 框架，当程序成为 Leader 时，直接执行 <a href=https://github.com/rancher/rancher/blob/v2.9.0-rc1/pkg/wrangler/context.go#L175>OnLeader</a> 回调函数启动一系列业务逻辑。而当程序还没被选为 Leader 时，只初始化 Informer Cache 等初始化步骤，不启动 Sync 相关逻辑。</p><p>通常 <code>sample-controller</code> 或其他简单的 Controller 在 Worker Start 执行完之后，会加一个 <code>&lt;-ctx.Done()</code> 阻塞（<a href=https://github.com/kubernetes/sample-controller/blob/master/controller.go#L182>代码位置</a>），遇到 Context Cancel 后直接结束运行。但如果加了 Leader Election 机制，当 Context Cancel 时是由 Leader Election 的 <code>OnStoppedLeading</code> 回调函数结束运行并释放 Lease 锁，所以 <code>main</code> 函数可以改为使用 <code>select {}</code> 阻塞，否则程序在 Context Cancel 时 Lease 锁还没来得及释放就由 <code>main</code> 函数结束运行了。</p><p>所以读到这里，可以得出结论，就算设置数量特别多的 Replicas，实际上依旧只有一个 Controller Pod 在执行真正的 Sync 逻辑，而其他 Pod 只是在观望，或者只提供一些 Web Server 功能。如果想让多个冗余 Pod 分别 Sync 不同的资源更新，需要设计一个更复杂的锁，而这又会增加一定的 API Server 请求数量……</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://blog.starry-s.moe/tags/kubernetes/>Kubernetes</a></li><li><a href=https://blog.starry-s.moe/tags/leader-election/>Leader Election</a></li></ul><nav class=paginav><a class=prev href=https://blog.starry-s.moe/posts/2024/container-network-2/><span class=title>« 上一页</span><br><span>再探容器网络</span>
</a><a class=next href=https://blog.starry-s.moe/posts/2024/spring-2024/><span class=title>下一页 »</span><br><span>摄影日记 - 2024 春</span></a></nav></footer><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//starry-s-blog.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></article></main><footer class=footer><span>© 2016 - 2025 STARRY-S | <a href=https://creativecommons.org/licenses/by-nc-nd/4.0/>CC BY-NC-ND 4.0</a> | Hosted on <a href=https://pages.github.com>GitHub Pages</a><br></span>·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>const pathname=window.location.pathname,images=document.querySelectorAll("img");Array.from(images).forEach(e=>{if(!pathname.includes("/posts/"))return;if(e.src.indexOf("images")<0)return;e.addEventListener("load",()=>fitImage(e)),e.complete&&e.naturalWidth!==0&&fitImage(e)});function fitImage(e){e.style.marginLeft="auto",e.style.marginRight="auto",e.naturalWidth/e.naturalHeight<1.1&&(e.style.maxWidth="60%",e.style.height="auto")}</script><script src=/js/home-info.js></script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="复制";function s(){t.innerHTML="已复制！",setTimeout(()=>{t.innerHTML="复制"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>